{"ast":null,"code":"\"use strict\";\n\nObject.defineProperty(exports, \"__esModule\", {\n  value: true\n});\nexports.GridFSBucketWriteStream = void 0;\nconst stream_1 = require(\"stream\");\nconst bson_1 = require(\"../bson\");\nconst error_1 = require(\"../error\");\nconst write_concern_1 = require(\"./../write_concern\");\n/**\n * A writable stream that enables you to write buffers to GridFS.\n *\n * Do not instantiate this class directly. Use `openUploadStream()` instead.\n * @public\n */\nclass GridFSBucketWriteStream extends stream_1.Writable {\n  /**\n   * @param bucket - Handle for this stream's corresponding bucket\n   * @param filename - The value of the 'filename' key in the files doc\n   * @param options - Optional settings.\n   * @internal\n   */\n  constructor(bucket, filename, options) {\n    super();\n    /**\n     * The document containing information about the inserted file.\n     * This property is defined _after_ the finish event has been emitted.\n     * It will remain `null` if an error occurs.\n     *\n     * @example\n     * ```ts\n     * fs.createReadStream('file.txt')\n     *   .pipe(bucket.openUploadStream('file.txt'))\n     *   .on('finish', function () {\n     *     console.log(this.gridFSFile)\n     *   })\n     * ```\n     */\n    this.gridFSFile = null;\n    options = options ?? {};\n    this.bucket = bucket;\n    this.chunks = bucket.s._chunksCollection;\n    this.filename = filename;\n    this.files = bucket.s._filesCollection;\n    this.options = options;\n    this.writeConcern = write_concern_1.WriteConcern.fromOptions(options) || bucket.s.options.writeConcern;\n    // Signals the write is all done\n    this.done = false;\n    this.id = options.id ? options.id : new bson_1.ObjectId();\n    // properly inherit the default chunksize from parent\n    this.chunkSizeBytes = options.chunkSizeBytes || this.bucket.s.options.chunkSizeBytes;\n    this.bufToStore = Buffer.alloc(this.chunkSizeBytes);\n    this.length = 0;\n    this.n = 0;\n    this.pos = 0;\n    this.state = {\n      streamEnd: false,\n      outstandingRequests: 0,\n      errored: false,\n      aborted: false\n    };\n    if (!this.bucket.s.calledOpenUploadStream) {\n      this.bucket.s.calledOpenUploadStream = true;\n      checkIndexes(this).then(() => {\n        this.bucket.s.checkedIndexes = true;\n        this.bucket.emit('index');\n      }, () => null);\n    }\n  }\n  /**\n   * @internal\n   *\n   * The stream is considered constructed when the indexes are done being created\n   */\n  _construct(callback) {\n    if (this.bucket.s.checkedIndexes) {\n      return process.nextTick(callback);\n    }\n    this.bucket.once('index', callback);\n  }\n  /**\n   * @internal\n   * Write a buffer to the stream.\n   *\n   * @param chunk - Buffer to write\n   * @param encoding - Optional encoding for the buffer\n   * @param callback - Function to call when the chunk was added to the buffer, or if the entire chunk was persisted to MongoDB if this chunk caused a flush.\n   */\n  _write(chunk, encoding, callback) {\n    doWrite(this, chunk, encoding, callback);\n  }\n  /** @internal */\n  _final(callback) {\n    if (this.state.streamEnd) {\n      return process.nextTick(callback);\n    }\n    this.state.streamEnd = true;\n    writeRemnant(this, callback);\n  }\n  /**\n   * Places this write stream into an aborted state (all future writes fail)\n   * and deletes all chunks that have already been written.\n   */\n  async abort() {\n    if (this.state.streamEnd) {\n      // TODO(NODE-3485): Replace with MongoGridFSStreamClosed\n      throw new error_1.MongoAPIError('Cannot abort a stream that has already completed');\n    }\n    if (this.state.aborted) {\n      // TODO(NODE-3485): Replace with MongoGridFSStreamClosed\n      throw new error_1.MongoAPIError('Cannot call abort() on a stream twice');\n    }\n    this.state.aborted = true;\n    await this.chunks.deleteMany({\n      files_id: this.id\n    });\n  }\n}\nexports.GridFSBucketWriteStream = GridFSBucketWriteStream;\nfunction handleError(stream, error, callback) {\n  if (stream.state.errored) {\n    process.nextTick(callback);\n    return;\n  }\n  stream.state.errored = true;\n  process.nextTick(callback, error);\n}\nfunction createChunkDoc(filesId, n, data) {\n  return {\n    _id: new bson_1.ObjectId(),\n    files_id: filesId,\n    n,\n    data\n  };\n}\nasync function checkChunksIndex(stream) {\n  const index = {\n    files_id: 1,\n    n: 1\n  };\n  let indexes;\n  try {\n    indexes = await stream.chunks.listIndexes().toArray();\n  } catch (error) {\n    if (error instanceof error_1.MongoError && error.code === error_1.MONGODB_ERROR_CODES.NamespaceNotFound) {\n      indexes = [];\n    } else {\n      throw error;\n    }\n  }\n  const hasChunksIndex = !!indexes.find(index => {\n    const keys = Object.keys(index.key);\n    if (keys.length === 2 && index.key.files_id === 1 && index.key.n === 1) {\n      return true;\n    }\n    return false;\n  });\n  if (!hasChunksIndex) {\n    await stream.chunks.createIndex(index, {\n      ...stream.writeConcern,\n      background: true,\n      unique: true\n    });\n  }\n}\nfunction checkDone(stream, callback) {\n  if (stream.done) {\n    return process.nextTick(callback);\n  }\n  if (stream.state.streamEnd && stream.state.outstandingRequests === 0 && !stream.state.errored) {\n    // Set done so we do not trigger duplicate createFilesDoc\n    stream.done = true;\n    // Create a new files doc\n    const gridFSFile = createFilesDoc(stream.id, stream.length, stream.chunkSizeBytes, stream.filename, stream.options.contentType, stream.options.aliases, stream.options.metadata);\n    if (isAborted(stream, callback)) {\n      return;\n    }\n    stream.files.insertOne(gridFSFile, {\n      writeConcern: stream.writeConcern\n    }).then(() => {\n      stream.gridFSFile = gridFSFile;\n      callback();\n    }, error => handleError(stream, error, callback));\n    return;\n  }\n  process.nextTick(callback);\n}\nasync function checkIndexes(stream) {\n  const doc = await stream.files.findOne({}, {\n    projection: {\n      _id: 1\n    }\n  });\n  if (doc != null) {\n    // If at least one document exists assume the collection has the required index\n    return;\n  }\n  const index = {\n    filename: 1,\n    uploadDate: 1\n  };\n  let indexes;\n  try {\n    indexes = await stream.files.listIndexes().toArray();\n  } catch (error) {\n    if (error instanceof error_1.MongoError && error.code === error_1.MONGODB_ERROR_CODES.NamespaceNotFound) {\n      indexes = [];\n    } else {\n      throw error;\n    }\n  }\n  const hasFileIndex = !!indexes.find(index => {\n    const keys = Object.keys(index.key);\n    if (keys.length === 2 && index.key.filename === 1 && index.key.uploadDate === 1) {\n      return true;\n    }\n    return false;\n  });\n  if (!hasFileIndex) {\n    await stream.files.createIndex(index, {\n      background: false\n    });\n  }\n  await checkChunksIndex(stream);\n}\nfunction createFilesDoc(_id, length, chunkSize, filename, contentType, aliases, metadata) {\n  const ret = {\n    _id,\n    length,\n    chunkSize,\n    uploadDate: new Date(),\n    filename\n  };\n  if (contentType) {\n    ret.contentType = contentType;\n  }\n  if (aliases) {\n    ret.aliases = aliases;\n  }\n  if (metadata) {\n    ret.metadata = metadata;\n  }\n  return ret;\n}\nfunction doWrite(stream, chunk, encoding, callback) {\n  if (isAborted(stream, callback)) {\n    return;\n  }\n  const inputBuf = Buffer.isBuffer(chunk) ? chunk : Buffer.from(chunk, encoding);\n  stream.length += inputBuf.length;\n  // Input is small enough to fit in our buffer\n  if (stream.pos + inputBuf.length < stream.chunkSizeBytes) {\n    inputBuf.copy(stream.bufToStore, stream.pos);\n    stream.pos += inputBuf.length;\n    process.nextTick(callback);\n    return;\n  }\n  // Otherwise, buffer is too big for current chunk, so we need to flush\n  // to MongoDB.\n  let inputBufRemaining = inputBuf.length;\n  let spaceRemaining = stream.chunkSizeBytes - stream.pos;\n  let numToCopy = Math.min(spaceRemaining, inputBuf.length);\n  let outstandingRequests = 0;\n  while (inputBufRemaining > 0) {\n    const inputBufPos = inputBuf.length - inputBufRemaining;\n    inputBuf.copy(stream.bufToStore, stream.pos, inputBufPos, inputBufPos + numToCopy);\n    stream.pos += numToCopy;\n    spaceRemaining -= numToCopy;\n    let doc;\n    if (spaceRemaining === 0) {\n      doc = createChunkDoc(stream.id, stream.n, Buffer.from(stream.bufToStore));\n      ++stream.state.outstandingRequests;\n      ++outstandingRequests;\n      if (isAborted(stream, callback)) {\n        return;\n      }\n      stream.chunks.insertOne(doc, {\n        writeConcern: stream.writeConcern\n      }).then(() => {\n        --stream.state.outstandingRequests;\n        --outstandingRequests;\n        if (!outstandingRequests) {\n          checkDone(stream, callback);\n        }\n      }, error => handleError(stream, error, callback));\n      spaceRemaining = stream.chunkSizeBytes;\n      stream.pos = 0;\n      ++stream.n;\n    }\n    inputBufRemaining -= numToCopy;\n    numToCopy = Math.min(spaceRemaining, inputBufRemaining);\n  }\n}\nfunction writeRemnant(stream, callback) {\n  // Buffer is empty, so don't bother to insert\n  if (stream.pos === 0) {\n    return checkDone(stream, callback);\n  }\n  ++stream.state.outstandingRequests;\n  // Create a new buffer to make sure the buffer isn't bigger than it needs\n  // to be.\n  const remnant = Buffer.alloc(stream.pos);\n  stream.bufToStore.copy(remnant, 0, 0, stream.pos);\n  const doc = createChunkDoc(stream.id, stream.n, remnant);\n  // If the stream was aborted, do not write remnant\n  if (isAborted(stream, callback)) {\n    return;\n  }\n  stream.chunks.insertOne(doc, {\n    writeConcern: stream.writeConcern\n  }).then(() => {\n    --stream.state.outstandingRequests;\n    checkDone(stream, callback);\n  }, error => handleError(stream, error, callback));\n}\nfunction isAborted(stream, callback) {\n  if (stream.state.aborted) {\n    process.nextTick(callback, new error_1.MongoAPIError('Stream has been aborted'));\n    return true;\n  }\n  return false;\n}","map":null,"metadata":{},"sourceType":"script"}